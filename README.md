# ‚ö° Otimiza√ß√£o de Join com PySpark

Este projeto tem como objetivo explorar o potencial do PySpark para processar grandes volumes de dados de forma eficiente, por meio da leitura, transforma√ß√£o e jun√ß√£o de arquivos Parquet. O foco est√° na **otimiza√ß√£o de desempenho**, utilizando t√©cnicas como `repartition`, `coalesce` e an√°lise com `EXPLAIN`.

## üìÇ Dados Utilizados
- `videos-preparados.snappy.parquet`
- `video-comments-tratados.snappy.parquet`

## üß™ Etapas Realizadas

1. **Leitura dos dados**  
   - Carregamento dos dois arquivos Parquet em DataFrames: `df_video` e `df_comments`.

2. **Cria√ß√£o de tabelas tempor√°rias**  
   - Registro dos DataFrames como views tempor√°rias no Spark SQL para facilitar consultas.

3. **Join inicial com Spark SQL**  
   - Realiza√ß√£o do `JOIN` entre v√≠deos e coment√°rios por meio de comandos SQL.

4. **Aplica√ß√£o de particionamento**  
   - Repeti√ß√£o das etapas anteriores utilizando `repartition` e `coalesce` para testar diferentes estrat√©gias de particionamento.

5. **An√°lise de plano de execu√ß√£o**  
   - Utiliza√ß√£o de `EXPLAIN` para compreender o plano de execu√ß√£o dos joins e identificar gargalos de performance.

6. **Otimiza√ß√£o do join final**  
   - Aplica√ß√£o de filtros antes do join para trabalhar apenas com os dados necess√°rios.
   - Uso combinado de particionamento adequado e sele√ß√£o de colunas relevantes.

7. **Exporta√ß√£o do resultado**  
   - Salvamento do resultado final otimizado como `join-videos-comments-otimizado` no formato Parquet.

## üí° Tecnologias
- PySpark (Spark SQL e DataFrame API)
- Python (executado no Google Colab ou ambiente local com PySpark configurado)
- Arquivos `.parquet` com compress√£o Snappy

